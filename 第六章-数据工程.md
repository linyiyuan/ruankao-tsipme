<!--
 * @Author: linyiyuan linyiyuann@gmail.com
 * @Date: 2024-10-24 20:18:01
 * @LastEditors: linyiyuan linyiyuann@gmail.com
 * @LastEditTime: 2024-10-29 16:08:20
 * @FilePath: \ruankao-tsipme\第六章-信息工程.md
 * @Description: 这是默认设置,请设置`customMade`, 打开koroFileHeader查看配置 进行设置: https://github.com/OBKoro1/koro1FileHeader/wiki/%E9%85%8D%E7%BD%AE
-->
# 数据采集和预处理
  1. 数据工程是信息系统的基础工程。它的相关能力是其建设数据要素的关键，是组织数据资源化、数据标准化、数据资产化、数据价值化的重要手段。
  2. 采集的数据类型包括结构化数据、半结构化数据、非结构化数据。
  3. 数据采集的方法可分为传感器采集、系统日志采集、网络采集和其他数据采集等。
    - 传感器采集是通过传感器感知相应的信息，并将这些信息按一定规律变换成电信号或其他所需的信息输出，从而获取相关数据，是目前应用非常广泛的一种采集方式。

    - 系统日志采集是通过平台系统读取、收集日志文件变化。它一般为流式数据，数据量非常庞大。

    - 网络采集是指通过互联网公开采集接口或者网络爬虫等方式从互联网或特定网络上获取大量数据信息的方式，是实现互联网数据或特定网络采集的主要方式。数据采集接口一般通过应用程序接口（API）的方式进行采集。
  4. 数据的预处理一般采用数据清洗的方法来实现。数据预处理是一个去除数据集重复记录，发现并纠正数据错误，并将数据转换成符合标准的过程，从而使数据实现准确性、完整性、一致性、唯一性、适时性、有效性等。一般来说，数据预处理主要包括数据分析、数据检测和数据修正3个步骤。
  5. 数据预处理方法
      1. 缺失数据的预处理
         - 删除缺失值，是最常见的、简单有效的方法，当样本数很多的时候，并且出现缺失值的样本占整个样本的比例相对较小时，可以将有缺失值的样本直接丢弃。
         - 均值填补法，是根据缺失值的属性相关系数最大的那个属性把数据分成几个组，再分别计算每个组的均值，用均值代替缺失数值。
         - 热卡填补法，通过在数据库中找到一个与包含缺失值变量最相似的对象，然后采用相似对象的值进行数据填充。
         - 缺失数据预处理的其他方法还有最近距离决定填补法、回归填补法、多重填补法、K-最近邻法、有序最近邻法、基于贝叶斯的方法等。
      2. 异常数据的预处理
          对于异常数据或有噪声的数据，如超过明确取值范围的数据、离群点数据，可以采用分箱法和回归法来进行处理。

          分箱法通过考察数据的"近邻"（即周围的值）来平滑处理有序的数据值，这些有序的值被分布到一些"桶"或"箱"中，进行局部光滑。一般而言，宽度越大，数据预处理的效果越好。
          回归法用一个函数拟合数据来光滑数据，消除噪声。线性回归涉及找出拟合两个属性（或变量）的"最佳"直线，使得一个属性能够预测另一个。
      3. 不一致数据的预处理
          这一类数据的清洗可以使用人工修改，也可以借助工具来找到违反限制的数据，如知道数据的函数依赖关系，可以通过函数关系修改属性值。但是大部分的不一致情况都需要进行数据变换，即定义一系列的变换纠正数据，有一些商业工具可以提供数据变换的功能，例如数据迁移工具和ETL工具等。
     4. 重复数据的预处理
        去除重复值的操作一般最后进行，可以使用Excel、VBA （VisualBasic宏语言）、Python等工具处理。
     5. 格式不符数据的预处理
        一般需要将不同类型的数据内容清洗成统一类型的文件和统一格式，如将TXT、CSV、Excek HTML以及PDF清洗成统一的Excel文件，将显示不一致的时间、日期、数值或者内容中有空格、单引号、双引号等情况进行格式的统一调整。
# 数据存储及管理
  1. 存储介质的类型:磁带、光盘、磁盘、内存、闪存、云存储等。
  2. 数据存储的形式:文件存储、块存储和对象存储。
  3. 存储管理的主要内容:
     1. 资源调度管理:添加或删除存储节点，编辑存储节点的信息，设定某类型存储资源属于某个节点，或者设定这些资源比较均衡地存储到节点上。
     2. 存储资源管理:是一类应用程序，它们管理和监控物理和逻辑层次上的存储资源，从而简化资源管理，提高数据的可用性。
     3. 负载均衡管理:为了避免存储资源由于资源类型、服务器访问频率和时间不均衡造成浪费或形成系统瓶颈而平衡负载的技术。
     4. 安全管理:主要是防止恶意用户攻击系统或窃取数据。
  4. 数据归档是将不活跃的冷数据从可立即访问的存储介质迁移到查询性能较低、低成本、大容量的存储介质中，这一过程是可逆的，即归档的数据可以恢复到原存储介质中。
      在开展数据归档活动时，有以下3点值得注意：
      - 数据归档一般只在业务低峰期执行。
      - 数据归档之后，将会删除生产数据库的数据，造成数据空洞，若长时间没有新的数据填充，会造成空间浪费的情况。
      - 如果数据归档影响了线上业务，一定要及时止损，结束数据归档，进行问题复盘，及时找到问题和解决方案。
  5. 最常见的4种数据备份结构:DAS备份结构、基于LAN的备份结构、LAN-FREE备份结构和SERVER-FREE备份结构。
  6. 备份策略是指确定需要备份的内容、备份时间和备份方式。主要有3种备份策略:完全备份、差分备份和增量备份。
      1. 完全备份:每次都对需要进行备份的数据进行全备份。当数据丢失时，用完全备份下来的数据进行恢复即可。缺点:会占用较多的服务器、网络等资源;二是有大量的数据是重复的，对备份介质资源的消耗较大。
      2. 差分备份:每次所备份的数据只是相对上一次完全备份之后发生变化的数据。优点:与完全备份相比，所需时间短，节省存储空间，恢复很方便。
      3. 增量备份:每次所备份的数据只是相对于上一次备份后改变的数据。优点:没有重复的备份数据，节省存储空间，缩短备份时间;缺点:进行数据恢复时比较复杂，可靠性没有完全备份和差分备份高。
   7. 数据备份是数据容灾的基础  衡量容灾系统的两个主要指标:RPO（恢复点目标）和RTO（恢复时间目标），其中RPO代表了当灾难发生时允许丢失的数据量，而RTO则代表了系统恢复的时间。
   8. 数据容灾的关键技术：
       1. 远程镜像技术:在主数据中心和备份中心之间进行数据备份时用到的远程复制技术。镜像是在两个或多个磁盘子系统上产生同一个数据镜像视图的数据存储过程，一个称为主镜像;另一个称为从镜像。按主从镜像所处的位置分为本地镜像和远程镜像。在灾难发生时，存储在异地的数据不会受到影响。
       2. 快照技术:所谓快照，就是关于指定数据集合的一个完全可用的复制，该复制是相应数据在某个时间点（复制开始的时间点）的映像。快照的作用:①能够进行在线数据恢复，可以将数据恢复成快照产生时间点时的状态;②为用户提供另外一个数据访问通道。
    
# 数据治理和建模
  1. 元数据是关于数据的数据。在信息技术及其服务行业，往往被定义为提供关于信息资源或数据的一种结构化数据，是对信息资源的结构化描述。其实质是用于描述信息资源或数据的内容、覆盖范围、质量、管理方式、数据的所有者、数据的提供方式等有关的信息。
  2. 数据标准化:
      数据标准化主要为复杂的信息表达、分类和定位建立相应的原则和规范，使其简单化、结构化和标准化，从而实现信息的可理解、可比较和可共享，为信息在异构系统之间实现语义互操作提供基础支撑。
      数据标准化的主要内容包括元数据标准化、数据元标准化、数据模式标准化和数据分类与编码标准化。
      数据标准化阶段的具体过程包括确定数据需求、制定数据标准、批准数据标准和实施数据标准。
  3. 数据质量:
      数据质量指在特定的业务环境下，数据满足业务运行、管理与决策的程度，是保证数据应用效果的基础。
      衡量数据质量的指标体系包括完整性、规范性、一致性、准确性、唯一性、及时性等。数据质量是一个广义的概念，是数据产品满足指标、状态和要求能力的特征总和。
        1. 数据质量描述，数据质量可以通过数据质量元素来描述。
        2. 数据质量评价过程，是产生和报告数据质量结果的一系列步骤。
        3. 数据质量评价方法，分为直接评价法和间接评价法。
            直接评价法通过将数据与内部或外部的参照信息（如理论值等）进行对比来确定数据质量。
            间接评价法利用数据相关信息（如对数据源、采集方法等的描述）推断或评估数据质量。
        4. 数据质量控制，分成前期控制和后期控制两大部分。
            前期控制包括数据录入前的质量控制、数据录入过程中的实时质量控制。
            后期控制为数据录入完成后的后处理质量控制与评价。
  4. 根据模型应用的目的不同，可以将数据模型划分为3类:概念模型、逻辑模型和物理模型。
  5. 概念模型，也称为信息模型，它是按用户的观点来对数据和信息建模。这种模型的基本元素包括:实体、属性、域、键、关联。
  6. 逻辑模型，是在概念模型的基础上确定模型的数据结构，主要有层次模型、网状模型、关系模型、面向对象模型和对象关系模型。其中，关系模型是目前最重要的一种逻辑数据模型，它的基本元素包括关系、关系的属性、视图等。
      关系模型是在概念模型的基础上构建的，因此这类模型的基本元素与概念模型中的基本元素存在一定的对应关系。
      关系模型的数据操作主要包括查询、插入、删除和更新数据，这些操作必须满足关系的完整性约束条件。关系的完整性约束包括三大类型:实体完整性、参照完整性和用户定义的完整性。其中，实体完整性、参照完整性是关系模型必须满足的完整性约束条件，用户定义的完整性是应用领域需要遵照的约束条件，体现了具体领域中的语义约束。
  7. 物理模型，在逻辑模型的基础上，考虑各种具体的技术实现因素，进行数据库体系结构设计，真正实现数据在数据库中的存放。内容包括确定所有的表和列，定义外键用于确定表之间的关系，基于性能的需求可能进行反规范化处理等;目标是用数据库模式来实现逻辑模型，以及真正地保存数据;基本元素包括表、字段、视图、索引、存储过程、触发器等，其中表、字段和视图等元素与逻辑模型中的基本元素有一定的对应关系。
  8. 数据建模的过程包括:数据需求分析、概念模型设计、逻辑模型设计和物理模型设计等。
      1. 数据需求分析。就是分析用户对数据的需要和要求，是数据建模的起点，通常不是单独进行的，而是融合在整个系统需求分析的过程之中。采用数据流图作为工具，
      描述系统中数据的流动和数据变化，强调数据流和处理过程。
      2. 概念模型设计。首先把用户数据应用需求抽象为信息世界的结构，下一步才能更好地、更准确地用某个DBMS来实现用户的这些需求。其任务是确定实体和数据及其关联。
      3. 逻辑模型设计。为了能够在具体的DBMS上实现用户的需求，必须在概念模型的基础上进行逻辑模型的设计。这种设计主要指关系模型结构的设计，设计任务就是将概念模型中的实体、属性和关联转换为关系模型结构中的关系模式。
      4. 物理模型设计。如果要将数据模型转换为真正的数据库结构，还需要针对具体的DBMS进行物理模型设计，使数据模型走向数据存储应用环节。这种模型考虑的主要问题包括命名、确定字段类型和编写必要的存储过程与触发器等。
# 数据仓库和数据资产
  1. 数据仓库是一个面向主题的、集成的、随时间变化的、包含汇总和明细的、稳定的历史数据集合。
  2. 数据仓库通常由以下组件构成:
      1. 数据源，是数据仓库系统的基础，是整个系统的数据源泉。
      2. 数据的存储与管理，是整个数据仓库系统的核心，是数据仓库真正的关键。针对现有各业务系统的数据，进行抽取、清理，并有效集成，按照主题进行组织。数据仓库按照数据的覆盖范围可以分为企业级数据仓库和部门级数据仓库（通常称为数据集市）。
      3. OLAP（联机分析处理）服务器，对分析需要的数据进行有效集成，按多维模型予以组织，以便进行多角度、多层次的分析，并发现趋势。其具体实现可以分为:ROLAP（关系数据的关系在线分析处理）、MOLAP（多维在线分析处理）和HOLAP（混合在线分析处理）。ROLAP基本数据和聚合数据均存放在RDBMS之中;MOLAP基本数据和聚合数据均存放于多维数据库中;HOLAP基本数据存放于RDBMS之中，聚合数据存放于多维数据库中。
      4. 前端工具，主要包括各种查询工具、报表工具、分析工具、数据挖掘工具以及各种基于数据仓库或数据集市的应用开发工具。其中，数据分析工具主要针对OLAP服务器，报表工具、数据挖掘工具主要针对数据仓库。
  3. 主题库建设是数据仓库建设的一部分，是为了便利工作、精准快速地反映工作对象全貌而建立的融合各类原始数据、资源数据等，围绕能标识组织、人员、产权、财务等的主题对象，长期积累形成的多种维度的数据集合。可采用多层级体系结构，即数据源层、构件层、主题库层。
  4. 数据资产管理:在数字时代，数据是一种重要的生产要素，把数据转化成可流通的数据要素，重点包含以下两个环节。
       1. 数字资源化，通过将原始数据转变为数据资源，使数据具备一定的潜在价值，是数据资产化的必要前提。以数据治理为工作重点，以提升数据质量、保障数据安全为目标，确保数据的准确性、一致性、时效性和完整性，推动数据内外部流通。
       2. 数据资产化，通过将数据资源转变为数据资产，使数据资源的潜在价值得以充分释放。以扩大数据资产的应用范围、显性化数据资产的成本与效益为工作重点，并使数据供给端与数据消费端之间形成良性反馈闭环。
      数据价值评估是数据资产管理的关键环节，是数据资产化的价值基线。
  5. 数据资源编目是实现数据资产管理的重要手段。数据资源目录体系设计包括概念模型设计和业务模型设计等，概念模型设计明确数据资源目录的构成要素，通过业务模型设计规范数据资源目录的业务框架。
  6. 数据资源目录的概念模型由以下要素构成:
      1. 数据资源目录，是站在全局视角对所拥有的全部数据资源进行编目，以便对数据资源进行管理、识别、定位、发现、共享的一种分类组织方法，从而达到对数据的浏览、查询、获取等目的。它分为3个层面:资源目录、资产目录和服务目录。
      2. 信息项，是将各类数据资源（如表、字段）以元数据流水账的形式清晰地反映出来，以便更好地了解、掌握和管理数据资源。它需要通过数据标识符挂接到对应的数据目录。常分为3种类型:数据资源信息项、数据资产信息项和数据服务信息项。
      3. 数据资源库，是存储各类数据资源的物理数据库，常分为专题数据资源库、主题数据资源库和基础数据资源库。
      4. 标准规范，包括数据资源元数据规范、编码规范、分类标准等相关标准。元数据规范描述数据资源所必须具备的特征要素;编码规范规定了数据资源目录相关编码的表示形式、结构和维护规则;分类标准规范了数据资源分类的原则和方法。
# 数据分析及应用
  1. 数据的分析及应用是数据要素价值实现环节的重要活动，是组织实施数据驱动发展的基础，通常涉及数据集成、数据挖掘、数据服务和数据可视化等。
  2. 数据集成就是将驻留在不同数据源中的数据进行整合，向用户提供统一的数据视图，使得用户能以透明的方式访问数据。这些数据源具有存储位置分散、数据类型异构、数据库产品多样等特点。
  3. 数据集成的常用方法： 
      1. 模式集成：也叫虚拟视图方法，是人们最早采用的数据集成方法，也是其他数据集成方法的基础。其基本思想是：在构建集成系统时，将各数据源共享的视图集成为全局模式，供用户透明地访问各数据源的数据。
      2. 复制集成：将数据源中的数据复制到相关的其他数据源上，并对数据源的整体一致性进行维护，从而提高数据的共享和利用效率。 
      3. 混合集成：为了提高中间件系统的性能，保留虚拟数据模式视图为用户所用，同时提供数据复制的方法。
  4. 常用的数据访问接口标准： 
      1. ODBC：当前被业界广泛接受的、用于数据库访问的应用程序编程接口（API），由应用程序接口、驱动程序管理器、驱动程序和数据源4个组件组成。
      2. JDBC：用于执行SQL语句的Java应用程序接口，它由Java语言编写的类和接口组成。
      3. OLE DB：一个基于组件对象模型（COM）的数据存储对象，能提供对所有类型数据的操作，甚至能在离线的情况下存取数据。 
      4. ADO：应用层的接口，它的应用场合非常广泛，不仅可用在高级编程语言环境，还可用在Web开发等领域。它是COM自动接口，几乎所有数据库工具、应用程序开发环境和脚本语言都可以访问这种接口
  5. Web Services技术
      WebServices技术是一个面向访问的分布式计算模型，是实现Web数据和信息集成的有效机制。它的本质是用一种标准化方式实现不同服务系统之间的互调或集成。它基于XML、SOAP（简单对象访问协议）、WSDL（Web服务描述语言）和UDDI（统一描述、发现和集成协议规范）等协议，开发、发布、发现和调用跨平台、跨系统的各种分布式应用。
      Web Services技术的三要素:
        1. WSDL:一种基于XML格式的关于Web服务的描述语言。
        2. SOAP:是消息传递的协议，它规定了WebServices之间是怎样传递信息的。
        3. UDDI:是一种创建注册服务的规范。简单地说，它用于集中存放和查找WSDL描述文件，起着目录服务器的作用，以便服务提供者注册发布Web
        Services，供使用者查找。
  6. 数据网格技术 
      数据网格是一种用于大型数据集的分布式管理与分析的体系结构，目标是实现对分布、异构的海量数据进行一体化存储、管理、访问、传输与服务，为用户提供数据访问接口和共享机制，统一、透明地访问和操作各个分布、异构的数据资源，提供管理、访问各种存储系统的方法，解决应用所面临的数据密集型网格计算问题。 

      数据网格的透明性体现为： 
         1. 分布透明性：用户感觉不到数据是分布在不同的地方的； 
         2. 异构透明性：用户感觉不到数据的异构性，感觉不到数据存储方式的不同、数据格式的不同、数据管理系统的不同等； 
         3. 数据位置透明性：用户不用知道数据源的具体位置，也没有必要了解数据源的具体位置；
         4. 数据访问方式透明性：不同系统的数据访问方式不同，但访问结果相同。
   7. 数据挖掘是指从大量数据中提取或"挖掘"知识，即从大量的、不完全的、有噪声的、模糊的、随机的实际数据中，提取隐含在其中的、人们不知道的、却是潜在有用的知识，它把人们从对数据的低层次的简单查询，提升到从数据库挖掘知识，提供决策支持的高度。
   8. 数据挖掘与传统数据分析存在较大的不同，主要表现在以下4个方面。 
        1. 两者分析对象的数据量有差异。数据挖掘所需的数据量比传统数据分析所需的数据量大。数据量越大，数据挖掘的效果越好。 
        2. 两者运用的分析方法有差异。传统数据分析主要运用统计学的方法手段对数据进行分析；数据挖掘综合运用数据统计、人工智能、可视化等技术对数据进行分析。 
        3. 两者分析侧重有差异。传统数据分析通常是回顾型和验证型的，通常分析已经发生了什么；数据挖掘通常是预测型和发现型的，预测未来的情况，解释发生的原因。 
        4. 两者成熟度不同。传统数据分析由于研究较早，其分析方法相当成熟；数据挖掘除基于统计学等方法外，部分方法仍处于发展阶段。
   9. 数据挖掘的主要任务:数据总结、关联分析、分类和预测、聚类分析和孤立点分析。
      聚类分析是按照某种相近程度度量方法，将数据分成一系列有意义的子集合，每一个集合中的数据性质相近，不同集合之间的数据性质相差较大。当要分析的数据缺乏描述信息，或者无法组织成任何分类模型时，可以采用聚类分析。
      数据库中的数据常有一些异常记录，与其他记录存在着偏差。孤立点分析（或称为离群点分析）就是从数据库中检测出偏差。
   10. 数据挖掘流程一般包括5个阶段:确定分析对象、数据准备、数据挖掘、结果评估与结果应用。其中，数据准备包括数据选择和数据预处理，数据预处理又包括数据清理、数据集成、数据变换和数据归约。数据挖掘过程细分为模型构建过程和挖掘处理过程。
   11. 数据服务主要包括数据目录服务、数据查询与浏览及下载服务、数据分发服务。
   12. 数据可视化主要分为7类:一维数据可视化、二维数据可视化、三维数据可视化、多维数据可视化、时态数据可视化、层次数据可视化和网络数据可视化。
# 数据脱敏和分类分级
  1. 敏感数据可以分为个人敏感数据、商业敏感数据、国家秘密数据等。为了更加有效地管理敏感数据，可以按敏感程度划分为5个等级:L1（公开）、L2（保密）、L3（机密）、L4（绝密）和L5（私密）。
  2. 数据脱敏是对各类数据所包含的自然人身份标识、用户基本资料等敏感信息进行模糊化、加扰、加密或转换后形成无法识别、无法推算演绎、无法关联分析原始用户身份标识等的新数据，这样就可以在非生产环境、非可控环境、生产环境、数据共享、数据发布等环境中安全地使用脱敏后的真实数据集。
  3. 数据脱敏方式包括可恢复与不可恢复两类。可恢复类指脱敏后的数据可通过一定的方式，恢复成原来的敏感数据，此类脱敏规则主要指各类加解密算法规则。不可恢复类指脱敏后的数据被脱敏的部分使用任何方式都不能恢复，一般可分为替换算法和生成算法两类。数据脱敏方式主要由应用场景决定，例如，对于发布数据场景，既要考虑直接表示信息，又要非表示信息，防止通过推算演绎、关联分析等手段，定位到用户身份。
  4. 数据脱敏原则主要包括算法不可逆原则、保持数据特征原则、保留引用完整性原则、规避融合风险原则、脱敏过程自动化原则和脱敏结果可重复原则等。
      1. 算法不可逆原则:是指除一些特定场合存在可恢复式数据复敏需求外，数据脱敏算法通常应当是不可逆的，必须防止使用非敏感数据推断、重建敏感原始数据。
      2. 保持数据特征原则:是指脱敏后的数据应具有原数据的特征，因为它们仍将用于开发或测试场合。带有数值分布范围、具有指定格式（如信用卡号前4位指代银行名称）的数据，在脱敏后应与原始信息相似。
      3. 保留引用完整性原则:是指数据的引用完整性应予以保留，如果被脱敏的字段是数据表主键，那么相关的引用记录必须同步更改。
      4. 规避融合风险原则:是指应当预判非敏感数据集多源融合可能造成的数据安全风险。对所有可能生成敏感数据的非敏感字段同样进行脱敏处理。
      5. 脱敏过程自动化原则:是指脱敏过程必须能够在规则的引导下自动化进行，才能达到可用性要求，更多的是强调不同环境的控制功能。
      6. 脱敏结果可重复原则:是指在某些场景下，对同一字段脱敏的每轮计算结果都相同或者都不同，以满足数据使用方可测性、模型正确性、安全性等指标的要求。
  5. 数据分类的两个要素:分类对象和分类依据。
      分类对象由若干个被分类的实体组成，分类依据取决于分类对象的属性或特征。分类应以相对最稳定的本质属性为依据，但是对具有交叉、双重或多重本质属性特征的信息进行分类，除了需要符合科学性、系统性等原则外，还应符合交叉性、双重或多重性的原则。
  6. 数据分级是指按照数据遭到破坏（包括攻击、泄露、篡改、非法使用等）后对国家安全、社会秩序、公共利益以及公民、法人和其他组织的合法权益（受侵害客体）的危害程度，对数据进行定级，主要是为数据全生命周期管理进行的安全策略制定。
      从国家数据安全角度出发，数据分级基本框架分为一般数据、重要数据、核心数据3个级别。